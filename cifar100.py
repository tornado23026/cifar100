# -*- coding: utf-8 -*-
"""CIFAR-100-Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mKNtuMcbzibOBDfp8H53wXwZljJsBcMg
"""

# Commented out IPython magic to ensure Python compatibility.
#@title (Hidden) Diagnostic Check
import os
import sys
import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from torchvision.datasets import CIFAR100
import torchvision.transforms as tt
from torchvision.utils import make_grid
from torch.utils.data.dataloader import DataLoader
from torch.utils.data import random_split,ConcatDataset

# %matplotlib inline

print(f"\nPython environment:\n{sys.version}")
print("\nStatistical packages loaded: ")
print(f"numpy: {np.__version__}")
print(f"pytorch: {torch.__version__}")

# Detect if a GPU is present with CUDA support
use_cuda = torch.cuda.is_available()
print(f"GPU Enabled: {use_cuda}")

print(f"\nRuntime location:")
try:
  from google.colab import drive
  is_google_colab = True
  print("Notebook is on Google CoLab")
except:
  is_google_colab = False
  print("Notebook is being run locally or through another source.")

seed = 1943
torch.manual_seed(seed)
np.random.seed(seed)
print(f"\nRandom Seed: {seed}")

# stats = ((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))
train_transform = tt.Compose([
    tt.RandomHorizontalFlip(),
    tt.RandomVerticalFlip(),
    tt.RandomCrop(32, padding = 4, padding_mode = "reflect"),
    tt.ColorJitter(brightness = 0.05, contrast = 0.05, saturation = 0.05, hue = 0.05),
    tt.RandomAffine(degrees = (-1, 1), translate = (0.01, 0.05)), 
    tt.ToTensor(),
    # tt.Normalize(*stats)
])

test_transform = tt.Compose([
    tt.ToTensor(),
    # tt.Normalize(*stats)
])

train = CIFAR100(download = True, root= "./data", transform = train_transform)
test = CIFAR100(root = "./data", train = False, transform = test_transform)

# checking the data
for image, label in train:
    print("Image shape: ",image.shape)
    print("Image tensor: ", image)
    print("Label: ", label)
    break

# checking the 100 different classes that should be present in the training set
print(len(set(train.classes)))

# check train / test datasets
# creates dictionaries of each class in train / test and how many images are in each class
train_items = dict()
test_items = dict()

for train_item in train:
    label = train.classes[train_item[1]]
    if label not in train_items:
        train_items[label] = 1
    else:
        train_items[label] += 1

for test_item in test:
    label = test.classes[test_item[1]]
    if label not in test_items:
        test_items[label] = 1
    else:
        test_items[label] += 1

# check train / test datasets continued
# running this code chunk should produce no output
for i in train_items:
    # train and test should both have 100 different classes
    if i not in test_items: 
        print(False)
        break
    # train should have 500 of each class and test should have 100 of each class
    if train_items[i] != 500 and test_items[i] != 100: 
        print(False)
        break

"""Technically we could split the training set further into a train-dev-test, but with only 600 images for each class (500 in training and 100 in test), it doesn't make that much sense to thin out the data further. """

BATCH_SIZE = 512
train_dl = DataLoader(train, BATCH_SIZE, num_workers = 2, pin_memory = True, shuffle = True)
test_dl = DataLoader(test, BATCH_SIZE, num_workers = 2, pin_memory = True)

def show_batch(dl):
    for batch in dl:
        images, labels = batch
        fig, ax = plt.subplots(figsize = (7.5, 7.5))
        ax.set_yticks([])
        ax.set_xticks([])
        ax.imshow(make_grid(images[:20], nrow = 5).permute(1, 2, 0))
        break

show_batch(train_dl)

def get_device():
    if torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")

def to_device(data,device):
    if isinstance(data,(list,tuple)):
        return [to_device(x,device) for x in data]
    return data.to(device,non_blocking=True)


class ToDeviceLoader:
    def __init__(self,data,device):
        self.data = data
        self.device = device
        
    def __iter__(self):
        for batch in self.data:
            yield to_device(batch,self.device)
            
    def __len__(self):
        return len(self.data)

device = get_device()
print(device)

train_dl = ToDeviceLoader(train_dl,device)
test_dl = ToDeviceLoader(test_dl,device)

def accuracy(predicted,actual):
    _, predictions = torch.max(predicted,dim=1)
    return torch.tensor(torch.sum(predictions==actual).item()/len(predictions))

class BaseModel(nn.Module):
    def training_step(self,batch):
        images,labels = batch
        out = self(images)
        loss = F.cross_entropy(out,labels)
        return loss
    
    def validation_step(self,batch):
        images,labels = batch
        out = self(images)
        loss = F.cross_entropy(out,labels)
        acc = accuracy(out,labels)
        return {"val_loss":loss.detach(),"val_acc":acc}
    
    def validation_epoch_end(self,outputs):
        batch_losses = [loss["val_loss"] for loss in outputs]
        loss = torch.stack(batch_losses).mean()
        batch_accuracy = [accuracy["val_acc"] for accuracy in outputs]
        acc = torch.stack(batch_accuracy).mean()
        return {"val_loss":loss.item(),"val_acc":acc.item()}
    
    def epoch_end(self, epoch, result):
        print("Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}".format(
            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))
   
def conv_block(in_channels, out_channels, pool=False):
    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), 
              nn.BatchNorm2d(out_channels), 
              nn.ReLU(inplace=True)]
    if pool: layers.append(nn.MaxPool2d(2))
    return nn.Sequential(*layers)
    

class ResNet152(BaseModel):
    def __init__(self, in_channels, num_classes):
#         super().__init__()
#         # Use a pretrained model
#         self.network = models.resnet34(pretrained=True)
#         # Replace last layer
#         num_ftrs = self.network.fc.in_features
#         self.network.fc = nn.Linear(num_ftrs, num_classes)
        super().__init__()
        
        self.conv1 = conv_block(in_channels, 64)
        self.conv2 = conv_block(64, 128, pool=True)
        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))
        
        self.conv3 = conv_block(128, 256, pool=True)
        self.conv4 = conv_block(256, 512, pool=True)
        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))
        self.conv5 = conv_block(512, 1028, pool=True)
        self.res3 = nn.Sequential(conv_block(1028, 1028), conv_block(1028, 1028))
        
        self.classifier = nn.Sequential(nn.MaxPool2d(2), 
                                        nn.Flatten(), 
                                        nn.Linear(1028, num_classes))
        
    
#     def forward(self, xb):
        
#         return torch.relu(self.network(xb))
    def forward(self, xb):
        out = self.conv1(xb)
        out = self.conv2(out)
        out = self.res1(out) + out
        out = self.conv3(out)
        out = self.conv4(out)
        out = self.res2(out) + out
        out = self.conv5(out)
        out = self.res3(out) + out

        out = self.classifier(out)
        return out

model = to_device(ResNet152(3, 100), device)
model

@torch.no_grad()
def evaluate(model,test_dl):
    model.eval()
    outputs = [model.validation_step(batch) for batch in test_dl]
    return model.validation_epoch_end(outputs)

def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']

def fit (epochs,train_dl,test_dl,model,optimizer,max_lr,weight_decay,scheduler,grad_clip=None):
    torch.cuda.empty_cache()
    
    history = []
    
    optimizer = optimizer(model.parameters(),max_lr,weight_decay=weight_decay)
    
    scheduler = scheduler(optimizer,max_lr,epochs=epochs,steps_per_epoch=len(train_dl))
    
    for epoch in range(epochs):
        model.train()
        
        train_loss = []
        
        lrs = []
        
        for batch in train_dl:
            loss = model.training_step(batch)
            
            train_loss.append(loss)
            
            loss.backward()
            
            if grad_clip:
                nn.utils.clip_grad_value_(model.parameters(),grad_clip)
            
            optimizer.step()
            optimizer.zero_grad()
            
            scheduler.step()
            lrs.append(get_lr(optimizer))
        result = evaluate(model,test_dl)
        result["train_loss"] = torch.stack(train_loss).mean().item()
        result["lrs"] = lrs
        
        model.epoch_end(epoch,result)
        history.append(result)
        
    return history

history = [evaluate(model,test_dl)]

print(history)

epochs = 50
optimizer = torch.optim.Adam
max_lr=0.01
grad_clip = 0.1
weight_decay = 1e-4
scheduler = torch.optim.lr_scheduler.OneCycleLR

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history += fit(epochs=epochs,train_dl=train_dl,test_dl=test_dl,model=model,optimizer=optimizer,max_lr=max_lr,grad_clip=grad_clip,
#               weight_decay=weight_decay,scheduler=torch.optim.lr_scheduler.OneCycleLR)

def plot_acc(history):
    plt.plot([x["val_acc"] for x in history],"-x")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")

def plot_loss(history):
    plt.plot([x.get("train_loss") for x in history], "-bx")
    plt.plot([x["val_loss"] for x in history],"-rx")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend(["train loss","val loss"])
    
def plot_lrs(history):
    plt.plot(np.concatenate([x.get("lrs",[]) for x in history]))
    plt.xlabel("Batch number")
    plt.ylabel("Learning rate")

plot_loss(history)

plot_acc(history)

plot_lrs(history)

"""lets shove this into the model for a bit longer (with a lower learning rate) cuz why the fuck not"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history += fit(epochs=epochs,train_dl=train_dl,test_dl=test_dl,model=model,optimizer=optimizer,max_lr=0.001,grad_clip=grad_clip,
#               weight_decay=weight_decay,scheduler=torch.optim.lr_scheduler.OneCycleLR)
